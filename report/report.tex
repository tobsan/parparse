\documentclass[a4paper,12pt,twosided]{report}
\usepackage[utf8]{inputenc}
\usepackage{datetime}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf}
\usepackage{listings}
\usepackage{float}
\lstnewenvironment{code}{\lstset{language=Haskell,basicstyle=\small}}{}

\newdateformat{monthdate}{\monthname[\THEMONTH] \THEYEAR}

\begin{document}

\include{titlepage}

\begin{abstract}
This is an abstract
\end{abstract}

\tableofcontents

%
% NEW CHAPTER
%

\chapter{Background}

\section{Introduction}
The topic of this thesis is to do \textbf{parsing} in an \textbf{incremental}
fashion that can easily be \textbf{parallelizable}, using a
\textbf{divide-and-conquer approach}. In this section, I will give a brief
explanation of the topics covered, and end with a motivation for why this is
interesting to do in the first place.

\subsection{Divide-and-conquer}
Add stuff here from section 2 in parparse paper.

\subsection{Incrementality}
Doing something incrementally means that one does it step by step, and not
longer than neccessary.

\subsection{Parallelism}
Why is parallelism interesting and how does it apply in this case?

\subsection{Parsing}
To parse is a to check if some given input corresponds to a certain language's
grammar, and in this thesis it will use \textbf{context-free grammars} for
programming languages.

\subsection{Motivation}
In compilers, lexing and parsing are the two first phases. The output of these
is an abstract syntax tree (AST) which is fed to the next phase of the compiler.
But an AST could also provide useful feedback for programmers, already in their
editor, if the code could be lexed and parsed fast enough. With a lexer and
parser that is incremental and that can also be parallelized could real-time
feedback in the form of an AST easily be provided to the programmer. Most
current text editors give syntax feedback based on regular expressions, which
does not yield any information about depth or the surrounding AST.

Something something about connecting to a type-checker.

\section{Lexing}
* Describe what lexing is
* Shortly describe LexGen and its relevance.

\section{Context-free grammars}
Give a light-weight description here.

% Citera/kolla automataboken hÃ¤r.
Formally, a context-free grammar is a 4-tuple: $G = (V, \Sigma, P, S)$. $V$ is
a set of non-terminals, or variables. $\Sigma$ is the set of terminal symbols,
describing the content that can be written in the language. $P$ is a 

\subsection{Backus-Naur Form}
% John Backus wrote a paper on this, cite it. See compling-essay.
Context-free grammars are often used to describe the syntax of programming
languages. Such descriptions are often given in a \textbf{labelled Backus-Naur
form}, where each rule is written on the following form:
$$Label. Variable ::= Production$$ % Something like that, at least.
% Cite BNFC
Grammars written in Labelled Backus-Naur Form are also used in the \textbf{BNF
Converter (BNFC)}, a lexer and parser generator tool developed at Chalmers.
Given such a grammar, BNFC generates, among other things, a lexer and a parser,
implemented in one of several languages, for the language described in that
grammar. According to documentation, usage of BNFC saves approx 90\% of
source code in writing a compiler front-end. 

\subsection{Chomsky Normal Form}
Chomsky Normal Form (CNF) is a subset of context-free grammars that was first
described by linguist Noam Chomsky. Productions in CNF are restricted to the
following forms:

\begin{figure}[h]
\begin{tabular}{l l l l}
    A & $\rightarrow$ & BC, & A variable, B and C productions \\
    A & $\rightarrow$ & a, & A variable, a terminal symbol \\
\end{tabular}
\caption{Rules allowed in Chomsky Normal Form}
\end{figure}
Since grammars in CNF are restricted to branches or single terminal symbols,
they are well suited for usage in divide-and-conquer algorithms. There are
several existing algorithms to convert context-free grammars into CNF, so one
does not have to write their grammars in CNF in the first place.
% Cite Lange and Leiss, whose algorithm is used in BNFC.

\section{Parsing}
\subsection{CYK algorithm}
\subsection{Valiant}
\subsection{Improvement by Bernardy \& Claessen}
Running time analysis. Oracle for list.

\section{Dependently typed programming}
In dependently typed programming, types are dependent on values.

\subsection{Kinds, Types and Values}
Describe how kinds relate to types as types relate to values.

%
% NEW CHAPTER
%

\chapter{Implementation}
Before going into details about the implementation of this parser, there are a
couple of libraries and programming techniques one has to be familiar with
before moving forward. I will first describe those, and then move on to describe
changes to the lexer I inherited, and the implementation of the parser.

\section{Finger trees}
A finger tree is a finite data structure with logarithmic time access and
concatenation. The finger tree is similar to a general binary tree, where each
branch has a couple of \textit{fingers} (values) so that adding a new value does
not neccessarily add a new branch to the tree. % TODO: Referens till paper om finger trees

A Haskell implementation suitable for everyday needs exists in the
\texttt{Data.Sequence} package, and a more general structure is available in the
\texttt{Data.FingerTree} package. The more general one is the one that will be
used for this project, and is the one that was used for the LexGen project.

\subsection{Measuring and Monoids}
Two specific features in the general FingerTree data type are the need for
Measuring and Monoids. These are fundamental to the parser, so we will look more
deeply into them here.

A \textit{monoid} is a mathematical object with an identity element and an
associative operation. In Haskell, this is provided by writing instances of this
type class:
\begin{figure}[H]
% TODO: Src-referens
\begin{code}
class Monoid a where
    -- Identity of mappend
    mempty  :: a
    -- An associative operation
    mappend :: a -> a -> a
\end{code}
\caption{The Monoid type class}
\end{figure}
This means that anything that is a Monoid has an identity element (that can be
accessed with \textit{mempty}) and an associative operation to append monoids
together (\textit{mappend}).

% TODO: Explain what it means to be measured.
For the FingerTree type, anything that can be measured, has to also be a monoid.
This is ensured not by the data type itself, but by every function operating on
the finger tree. 
\begin{figure}[H]
% TODO: src-referens
\begin{code}
-- Things that can be measured
class Monoid v => Measured v a | a -> v where
    measure :: a -> v

-- FingerTrees are parametrized on both v (measures) and a (values)
data FingerTree v a

-- Create an empty finger tree
empty :: Measured v a => FingerTree v a
\end{code}
\caption{Measuring and the FingerTree type}
\end{figure}

This means that, in order to use the FingerTree type parametrized on some type
a, one has to have:
\begin{enumerate}
    \item{Another type v, such that $a \rightarrow v$ is a functional dependency}
    \item{A monoid instance of v}
    \item{An instance measured v a}
\end{enumerate}

\section{Lexing}
For lexing code into tokens, the results from the LexGen project was used.
However, some modifications had to be done to LexGen in order to be easily
generated from BNFC as an Alex file. One large change was done to the lexing
code, however, which is due to the fact that not only the lexer, but also the
parser, should work incrementally. In the LexGen code, the output structure is a
\texttt{Sequence} of tokens. Since \texttt{Sequence} is a less general
implementation of finger trees, they cannot be measured, and is therefore not as
suitable to use in an incremental setting.

\subsection{Position information}

\section{Parsing}
\subsection{BNFC}
\subsection{Pipeline of measures}
    An illustration would be good here
\subsection{Dependently typed programming with charts}
\subsection{Oracle and unsafePerformIO}

%
% NEW CHAPTER
%

\chapter{Results}

\section{Final product}
\subsection{Testing}

\section{Measurements}
How fast is it? What is the complexity?

%
% NEW CHAPTER
%
\chapter{Discussion}

\section{Pitfalls}
Look through the LOG to remember whatever happened. Describe sort of chronologically?

\subsection{Too many result branches}
Describe the reason for this, with the merge stuff.

\subsection{Position information}
* Tuple of monoids?
* RingP instance?
* Newtype for tuples?

\section{Future work}

%
% BIBLIOGRAPHY
%
\bibliographystyle{plain}
\bibliography{bibliography.bib}

%
% ANY APPENDICES HERE
%

\end{document}
