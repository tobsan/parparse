\documentclass[a4paper,12pt,twosided]{report}
\usepackage[utf8]{inputenc}
\usepackage{datetime}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf}

\newdateformat{monthdate}{\monthname[\THEMONTH] \THEYEAR}

\begin{document}

\include{titlepage}

\begin{abstract}
This is an abstract
\end{abstract}

\tableofcontents

%
% NEW CHAPTER
%

\chapter{Background}

\section{Introduction}
The topic of this thesis is to do \textbf{parsing} in an \textbf{incremental}
fashion that can easily be \textbf{parallelizable}, using a
\textbf{divide-and-conquer approach}. In this section, I will give a brief
explanation of the topics covered, and end with a motivation for why this is
interesting to do in the first place.

\subsection{Divide-and-conquer}
Add stuff here from section 2 in parparse paper.

\subsection{Incrementality}
Doing something incrementally means that one does it step by step, and not
longer than neccessary.

\subsection{Parallelism}
Why is parallelism interesting and how does it apply in this case?

\subsection{Parsing}
To parse is a to check if some given input corresponds to a certain language's
grammar, and in this thesis it will use \textbf{context-free grammars} for
programming languages.

\subsection{Motivation}
In compilers, lexing and parsing are the two first phases. The output of these
is an abstract syntax tree (AST) which is fed to the next phase of the compiler.
But an AST could also provide useful feedback for programmers, already in their
editor, if the code could be lexed and parsed fast enough. With a lexer and
parser that is incremental and that can also be parallelized could real-time
feedback in the form of an AST easily be provided to the programmer. Most
current text editors give syntax feedback based on regular expressions, which
does not yield any information about depth or the surrounding AST.

Something something about connecting to a type-checker.

\section{Lexing}
* Describe what lexing is
* Shortly describe LexGen and its relevance.

\section{Context-free grammars}
Give a light-weight description here.

% Citera/kolla automataboken h√§r.
Formally, a context-free grammar is a 4-tuple: $G = (V, \Sigma, P, S)$. $V$ is
a set of non-terminals, or variables. $\Sigma$ is the set of terminal symbols,
describing the content that can be written in the language. $P$ is a 

\subsection{Backus-Naur Form}
% John Backus wrote a paper on this, cite it. See compling-essay.
Context-free grammars are often used to describe the syntax of programming
languages. Such descriptions are often given in a \textbf{labelled Backus-Naur
form}, where each rule is written on the following form:
$$Label. Variable ::= Production$$ % Something like that, at least.
% Cite BNFC
Grammars written in Labelled Backus-Naur Form are also used in the \textbf{BNF
Converter (BNFC)}, a lexer and parser generator tool developed at Chalmers.
Given such a grammar, BNFC generates, among other things, a lexer and a parser,
implemented in one of several languages, for the language described in that
grammar. According to documentation, usage of BNFC saves approx 90\% of
source code in writing a compiler front-end. 

\subsection{Chomsky Normal Form}
Chomsky Normal Form (CNF) is a subset of context-free grammars that was first
described by linguist Noam Chomsky. Productions in CNF are restricted to the
following forms:

\begin{figure}[h]
\begin{tabular}{l l l l}
    A & $\rightarrow$ & BC, & A variable, B and C productions \\
    A & $\rightarrow$ & a, & A variable, a terminal symbol \\
\end{tabular}
\caption{Rules allowed in Chomsky Normal Form}
\end{figure}
Since grammars in CNF are restricted to branches or single terminal symbols,
they are well suited for usage in divide-and-conquer algorithms. There are
several existing algorithms to convert context-free grammars into CNF, so one
does not have to write their grammars in CNF in the first place.
% Cite Lange and Leiss, whose algorithm is used in BNFC.

\section{Parsing}
\subsection{CYK algorithm}
\subsection{Valiant}
\subsection{Improvement by Bernardy \& Claessen}
Running time analysis. Oracle for list.

\section{Dependently typed programming}
What is dependently typed programming, and how can it be used in Haskell. How
about an example using vectors (standard example, sort of).
\subsection{Kinds, Types and Values}
Describe how kinds relate to types as types relate to values.

%
% NEW CHAPTER
%

\chapter{Implementation}

\section{Finger trees}
What are they?
\section{Measuring and Monoids}

\subsection{Pipeline of measures}
An illustration would be good here

\section{Lexing}
\subsection{LexGen -- Alex discrepancy}
\subsection{Position information}

\section{Parsing}
\subsection{BNFC}
\subsection{Dependently typed programming with charts}
\subsection{Oracle and unsafePerformIO}

%
% NEW CHAPTER
%

\chapter{Results}

\section{Final product}
\subsection{Testing}

\section{Measurements}
How fast is it? What is the complexity?

%
% NEW CHAPTER
%
\chapter{Discussion}

\section{Pitfalls}
Look through the LOG to remember whatever happened. Describe sort of chronologically?

\subsection{Too many result branches}
Describe the reason for this, with the merge stuff.

\subsection{Position information}
* Tuple of monoids?
* RingP instance?
* Newtype for tuples?

\section{Future work}

%
% BIBLIOGRAPHY
%
\bibliographystyle{plain}
\bibliography{bibliography.bib}

%
% ANY APPENDICES HERE
%

\end{document}
